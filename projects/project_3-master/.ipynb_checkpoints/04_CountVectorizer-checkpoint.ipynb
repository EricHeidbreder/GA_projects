{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "\n",
    "In this notebook, I'll be converting my reddit data into vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('./data/reddit_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, CountVectorizer wasn't considering band and bands the same word. Fixing that here\n",
    "posts['selftext'] = posts['selftext'].apply(lambda x: x.replace('bands', 'band') if 'bands' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       looking to find some crust punk peeps I can ha...\n",
       "1       Maybe I'm overthinking it, but is the line   \\...\n",
       "2       Im new to punk culture and have been hearing t...\n",
       "3       I (14f) wasn't punk last year, I was really in...\n",
       "4       Hello, I’ve got a project for a customized lea...\n",
       "                              ...                        \n",
       "3821    This album has been getting a lot of love and ...\n",
       "3822    I’m not the biggest fan of his music although ...\n",
       "3823    Hey everyone, im currently moving and trying t...\n",
       "3824    This band is overlooked. Give them some love p...\n",
       "3825    Huge fan of finding unknown band in this space...\n",
       "Name: selftext, Length: 3826, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = posts['selftext']\n",
    "y = posts['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=182, stratify=y) # our data is pretty close to even, but I still want to stratify just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'https',\n",
    "    'com',\n",
    "    'www',\n",
    "    'amp',\n",
    "    'like',\n",
    "    'just',\n",
    "    'spotify',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS.union(stop_words), # Thanks jonrsharpe on StackOverflow: https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list/24386751\n",
    "#                        max_df = 0.98,\n",
    "#                        min_df = 0.01,\n",
    "                                           \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvec = cvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_cvec.toarray(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I want to see what my most frequent words are to determine if I need to add any other words to my stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "top_words = {}\n",
    "\n",
    "# Loop through columns\n",
    "for i in X_train_df.columns:\n",
    "    # Save sum of each column in dictionary\n",
    "    top_words[i] = X_train_df[i].sum()\n",
    "    \n",
    "# top_words to dataframe sorted by highest occurence\n",
    "most_freq = pd.DataFrame(sorted(top_words.items(),\n",
    "                               key = lambda x: x[1], \n",
    "                               reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>album</td>\n",
       "      <td>2511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>punk</td>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>open</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>si</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>band</td>\n",
       "      <td>1609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>know</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>music</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pop</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ve</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>new</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1\n",
       "0  album  2511\n",
       "1   punk  2073\n",
       "2   open  2006\n",
       "3     si  1921\n",
       "4   band  1609\n",
       "5   know   801\n",
       "6  music   714\n",
       "7    pop   681\n",
       "8     ve   656\n",
       "9    new   625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2869, 19416)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poppunkers    0.500784\n",
       "punk          0.499216\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = [0.8, 0.6]\n",
    "min_df = [0.1, 0.05]\n",
    "ngram_range = [(1, 1), (1, 2)]\n",
    "max_features = [1000, 3000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer and Naive Bayes\n",
    "pipe_cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Setting my parameters\n",
    "params_cvec_nb = {\n",
    "    'cvec__max_df' : max_df,\n",
    "    'cvec__min_df' : min_df,\n",
    "    'cvec__max_features' : max_features,\n",
    "    'cvec__ngram_range' : ngram_range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer and Logistic Regression\n",
    "pipe_cvec_lr = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "params_cvec_lr = {\n",
    "    'cvec__max_df' : max_df,\n",
    "    'cvec__min_df' : min_df,\n",
    "    'cvec__max_features' : max_features,\n",
    "    'cvec__ngram_range' : ngram_range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf Vectorizer and Naive Bayes\n",
    "pipe_tvec_nb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "params_tvec_nb = {\n",
    "    'tvec__max_df' : max_df,\n",
    "    'tvec__min_df' : min_df,\n",
    "    'tvec__max_features' : max_features,\n",
    "    'tvec__ngram_range' : ngram_range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf Vectorizer and Logistic Regression\n",
    "pipe_tvec_lr = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "params_tvec_lr = {\n",
    "    'tvec__max_df' : max_df,\n",
    "    'tvec__min_df' : min_df,\n",
    "    'tvec__max_features' : max_features,\n",
    "    'tvec__ngram_range' : ngram_range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_nb = GridSearchCV(pipe_cvec_nb,\n",
    "                         param_grid=params_cvec_nb,\n",
    "                         cv=5,\n",
    "                         n_jobs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec_lr = GridSearchCV(pipe_cvec_lr,\n",
    "                         param_grid=params_cvec_lr,\n",
    "                         cv=5,\n",
    "                         n_jobs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tvec_nb = GridSearchCV(pipe_tvec_nb,\n",
    "                         param_grid=params_tvec_nb,\n",
    "                         cv=5,\n",
    "                         n_jobs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tvec_lr = GridSearchCV(pipe_tvec_lr,\n",
    "                         param_grid=params_tvec_lr,\n",
    "                         cv=5,\n",
    "                         n_jobs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=16,\n",
       "             param_grid={'cvec__max_df': [0.8, 0.6],\n",
       "                         'cvec__max_features': [1000, 3000, 2000],\n",
       "                         'cvec__min_df': [0.1, 0.05],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(max_iter=2000))]),\n",
       "             n_jobs=16,\n",
       "             param_grid={'cvec__max_df': [0.8, 0.6],\n",
       "                         'cvec__max_features': [1000, 3000, 2000],\n",
       "                         'cvec__min_df': [0.1, 0.05],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=16,\n",
       "             param_grid={'tvec__max_df': [0.8, 0.6],\n",
       "                         'tvec__max_features': [1000, 3000, 2000],\n",
       "                         'tvec__min_df': [0.1, 0.05],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('lr', LogisticRegression())]),\n",
       "             n_jobs=16,\n",
       "             param_grid={'tvec__max_df': [0.8, 0.6],\n",
       "                         'tvec__max_features': [1000, 3000, 2000],\n",
       "                         'tvec__min_df': [0.1, 0.05],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(max_df=0.6, max_features=1000, min_df=0.1,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec_nb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SCORES\n",
      "===================\n",
      "Count Vectorizer with Naive Bayes: 0.649355176019519\n",
      "Count Vectorizer with Logistic Regression: 0.7685604740327641\n",
      "Tfidf Vectorizer with Naive Bayes: 0.7159288950853956\n",
      "Tfidf Vectorizer with Logistic Regression: 0.7960962007668178\n",
      "TESTING SCORES\n",
      "==================\n",
      "Count Vectorizer with Naive Bayes: 0.6311389759665622\n",
      "Count Vectorizer with Logistic Regression: 0.7439916405433646\n",
      "Tfidf Vectorizer with Naive Bayes: 0.6677115987460815\n",
      "Tfidf Vectorizer with Logistic Regression: 0.7460815047021944\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING SCORES')\n",
    "print('===================')\n",
    "print(f'Count Vectorizer with Naive Bayes: {gs_cvec_nb.score(X_train, y_train)}')\n",
    "print(f'Count Vectorizer with Logistic Regression: {gs_cvec_lr.score(X_train, y_train)}')\n",
    "print(f'Tfidf Vectorizer with Naive Bayes: {gs_tvec_nb.score(X_train, y_train)}')\n",
    "print(f'Tfidf Vectorizer with Logistic Regression: {gs_tvec_lr.score(X_train, y_train)}')\n",
    "\n",
    "print('TESTING SCORES')\n",
    "print('==================')\n",
    "print(f'Count Vectorizer with Naive Bayes: {gs_cvec_nb.score(X_test, y_test)}')\n",
    "print(f'Count Vectorizer with Logistic Regression: {gs_cvec_lr.score(X_test, y_test)}')\n",
    "print(f'Tfidf Vectorizer with Naive Bayes: {gs_tvec_nb.score(X_test, y_test)}')\n",
    "print(f'Tfidf Vectorizer with Logistic Regression: {gs_tvec_lr.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
